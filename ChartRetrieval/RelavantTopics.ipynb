{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mleshashi/thesis-sharma/sraEnv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correctly formatted URL for a local Elasticsearch instance\n",
    "es = Elasticsearch([\"http://localhost:9200\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cluster_name': 'elasticsearch', 'status': 'yellow', 'timed_out': False, 'number_of_nodes': 1, 'number_of_data_nodes': 1, 'active_primary_shards': 1, 'active_shards': 1, 'relocating_shards': 0, 'initializing_shards': 0, 'unassigned_shards': 1, 'delayed_unassigned_shards': 0, 'number_of_pending_tasks': 0, 'number_of_in_flight_fetch': 0, 'task_max_waiting_in_queue_millis': 0, 'active_shards_percent_as_number': 50.0}\n"
     ]
    }
   ],
   "source": [
    "# Check the health of the cluster\n",
    "health = es.cluster.health()\n",
    "print(health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the entire text in each cell is displayed without truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Adjusting display width might not be necessary depending on your pandas version, but it's here for completeness\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Load your DataFrame\n",
    "pew = pd.read_csv('../dataset/pew_dataset/metadata.csv')\n",
    "statista = pd.read_csv('../dataset/statista_dataset/metadata.csv')\n",
    "columns = ['title','caption']\n",
    "\n",
    "# Filtering the DataFrame to include only the specified columns\n",
    "pew_df = pew[columns]\n",
    "statista_df = statista[columns]\n",
    "combined_df = pd.concat([pew_df, statista_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in pew_dataset: 1486\n",
      "Number of rows in statista_df: 27868\n",
      "Number of rows in statista_df: 29354\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows in pew_dataset: {len(pew_df)}\")\n",
    "print(f\"Number of rows in statista_df: {len(statista_df)}\")\n",
    "print(f\"Number of rows in statista_df: {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, body={\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"title\": {\"type\": \"text\"},\n",
    "                \"content\": {\"type\": \"text\"},\n",
    "                \"embedding\": {\"type\": \"dense_vector\", \"dims\": 384}\n",
    "            }\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Exists: True\n",
      "{'documents': {'aliases': {}, 'mappings': {'properties': {'content': {'type': 'text'}, 'embedding': {'type': 'dense_vector', 'dims': 384, 'index': True, 'similarity': 'cosine'}, 'title': {'type': 'text'}}}, 'settings': {'index': {'routing': {'allocation': {'include': {'_tier_preference': 'data_content'}}}, 'number_of_shards': '1', 'provided_name': 'documents', 'creation_date': '1709672865336', 'number_of_replicas': '1', 'uuid': '4U_yVTmYTziNKUy-6RPndg', 'version': {'created': '8500010'}}}}}\n"
     ]
    }
   ],
   "source": [
    "# Check if the index exists\n",
    "exists = es.indices.exists(index=index_name)\n",
    "print(f\"Index Exists: {exists}\")\n",
    "\n",
    "# Get index information (settings and mappings)\n",
    "if exists:\n",
    "    index_info = es.indices.get(index=index_name)\n",
    "    print(index_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29354/29354 [04:32<00:00, 107.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# indexing the documents into embeddings using Elasticsearch with its unique ID, ensuring no duplicates are created.\n",
    "for _, row in tqdm(combined_df.iterrows(), total=combined_df.shape[0]):\n",
    "    # Concatenate title and caption with a space or some delimiter\n",
    "    combined_text = f\"{row['title']}. {row['caption']}\"\n",
    "    \n",
    "    # Generate embedding for the combined text\n",
    "    embedding = model.encode(combined_text).tolist()\n",
    "\n",
    "    # Generate a unique ID for the document using a hash of the title and caption\n",
    "    unique_id = hashlib.sha256(combined_text.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    # Index the document with the combined embedding and use the unique_id as the document ID\n",
    "    es.index(index=index_name, id=unique_id, body={\n",
    "        \"title\": row['title'],\n",
    "        \"content\": row['caption'],\n",
    "        \"embedding\": embedding\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the index: 29348\n"
     ]
    }
   ],
   "source": [
    "doc_count = es.count(index=index_name)['count']\n",
    "print(f\"Number of documents in the index: {doc_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming initialization and index creation as before\n",
    "\n",
    "topics_df= pd.read_csv('../dataset/topics.csv')\n",
    "\n",
    "# Dictionary to hold total similarity scores for each topic\n",
    "topic_similarity_scores = {}\n",
    "\n",
    "for _, row in topics_df.iterrows():\n",
    "    topic = row['Topic']\n",
    "    topic_embedding = model.encode(topic).tolist()\n",
    "    script_query = {\n",
    "        \"script_score\": {\n",
    "            \"query\": {\"match_all\": {}},\n",
    "            \"script\": {\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "                \"params\": {\"query_vector\": topic_embedding}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = es.search(index=index_name, body={\n",
    "        \"size\": 10000,  # You might want to adjust this size based on your needs\n",
    "        \"query\": script_query,\n",
    "        \"_source\": {\"excludes\": [\"embedding\"]}\n",
    "    })\n",
    "\n",
    "    # Sum up the scores of all hits for this topic\n",
    "    total_score = sum(hit['_score'] for hit in response['hits']['hits'])\n",
    "    topic_similarity_scores[topic] = total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Relevant Topics Based on Similarity Score:\n",
      "Topic: Are gas prices too high?, Total Similarity Score: 12017.70943549993\n",
      "Topic: Are social networking sites good for our society?, Total Similarity Score: 11854.20004799998\n",
      "Topic: Are social media platforms doing enough to prevent cyberbullying?, Total Similarity Score: 11723.159291899972\n",
      "Topic: Should government spending be reduced?, Total Similarity Score: 11606.382903299978\n",
      "Topic: Should prescription drugs be advertised directly to consumers?, Total Similarity Score: 11604.013122200024\n",
      "Topic: Does legal prostitution increase human trafficking?, Total Similarity Score: 11567.266546400002\n",
      "Topic: Is capitalism the best form of economy?, Total Similarity Score: 11557.294478599968\n",
      "Topic: Do violent video games contribute to youth violence?, Total Similarity Score: 11515.799558000026\n",
      "Topic: Do we need cash?, Total Similarity Score: 11477.134343400057\n",
      "Topic: Does lowering the federal corporate income tax rate create jobs?, Total Similarity Score: 11427.947919100014\n"
     ]
    }
   ],
   "source": [
    "# Rank topics by their total similarity score\n",
    "sorted_topics = sorted(topic_similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Output the top 10 topics based on cumulative similarity scores\n",
    "print(\"Top 10 Relevant Topics Based on Similarity Score:\")\n",
    "for topic, score in sorted_topics[:10]:\n",
    "    print(f\"Topic: {topic}, Total Similarity Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top relevant topics have been saved as a DataFrame to ../dataset/TopRelevant_topics.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the sorted topics\n",
    "topics_df = pd.DataFrame(sorted_topics[:10], columns=['Topic', 'Total Similarity Score'])\n",
    "\n",
    "# Specify the directory and filename\n",
    "save_dir = \"../dataset\"  # Adjust the path as needed\n",
    "filename = \"TopRelevant_topics.csv\"\n",
    "save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "#os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "topics_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Top relevant topics have been saved as a DataFrame to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sraEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
