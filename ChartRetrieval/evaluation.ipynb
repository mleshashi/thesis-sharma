{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@3:  0.9514426589871553\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    \"\"\"\n",
    "    Calculate the Discounted Cumulative Gain (DCG) at a given rank k.\n",
    "\n",
    "    Parameters:\n",
    "    r (list): A list of relevance scores.\n",
    "    k (int): The number of results to consider for the DCG calculation.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated DCG value up to rank k.\n",
    "    \"\"\"\n",
    "    # Convert the relevance scores into a numpy array of floating point numbers and limit the array size to k.\n",
    "    r = np.asarray(r)[:k]\n",
    "\n",
    "    # Check if the array has any elements, i.e., whether it's not empty.\n",
    "    if r.size:\n",
    "        # Calculate DCG using the formula: sum(relevance_scores / log2(rank_positions)).\n",
    "        # The relevance scores are divided by the logarithm base 2 of their rank positions starting from 2.\n",
    "        # This applies a discounting factor to each score based on its position in the list:\n",
    "        # - The relevance of the first element is divided by log2(2),\n",
    "        # - The second by log2(3), and so on.\n",
    "        # This discounting reflects that elements at higher ranks (lower positions) are less valuable.\n",
    "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "\n",
    "    # Return 0 if the relevance score list is empty or if k is zero.\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Function to calculate the Normalized Discounted Cumulative Gain (NDCG) at a given rank k.\n",
    "def ndcg_at_k(r, k):\n",
    "    # First calculate the ideal DCG (maximum possible DCG) for the given relevance scores, sorted in descending order.\n",
    "    idcg = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    # Avoid division by zero; if the ideal DCG is 0, return 0.\n",
    "    if not idcg:\n",
    "        return 0.\n",
    "    # Calculate the actual DCG for the given order of relevance scores and divide by the ideal DCG to get NDCG.\n",
    "    return dcg_at_k(r, k) / idcg\n",
    "\n",
    "# Example Usage with document data\n",
    "documents = [\n",
    "    {'score': 1.7409, 'relevance': 2, 'completeness': 1},\n",
    "    {'score': 1.7334, 'relevance': 1, 'completeness': 0},\n",
    "    {'score': 1.7225, 'relevance': 2, 'completeness': 1},\n",
    "]\n",
    "\n",
    "# Convert the list of documents into relevance scores.\n",
    "# If you want to consider 'completeness', you can adjust the calculation here.\n",
    "relevance_scores = [(doc['relevance'] + doc['completeness'])/2 for doc in documents]\n",
    "\n",
    "# Print the NDCG for the top 3 and top 4 documents using the calculated relevance scores.\n",
    "print(\"NDCG@3: \", ndcg_at_k(relevance_scores, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9514426589871553)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "# we have ground-truth relevance of some answers to a query:\n",
    "true_relevance = np.asarray([[1.5, 0.5, 1.5]])\n",
    "# we predict some scores (relevance) for the answers\n",
    "scores = np.asarray([[1.7409, 1.7334, 1.7225]])\n",
    "ndcg_score(true_relevance, scores, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"model\": \"gpt-4o\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"What’s in this image?\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"data:image/jpeg;base64\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 300\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-4o',\n",
       " 'messages': [{'role': 'user',\n",
       "   'content': [{'type': 'text', 'text': 'What’s in this image?'},\n",
       "    {'type': 'image_url', 'image_url': {'url': 'data:image/jpeg;base64'}}]}],\n",
       " 'max_tokens': 300}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sraEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
